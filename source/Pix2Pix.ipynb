{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef239f07",
   "metadata": {},
   "source": [
    "### Setting the stage for building and training the Pix2Pix model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd856eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import statements\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76059cfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Paths to dataset directories\n",
    "\n",
    "train_path = \"sonar_data_train\" # Change to lidar_data_train accordingly\n",
    "test_path = \"sonar_data_test\" # Change to lidar_data_test accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a02bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualising the shape of a sample image from training set\n",
    "\n",
    "sample_img = tf.io.read_file(os.path.join(train_path, 'train_562.jpg'))\n",
    "sample_img = tf.io.decode_jpeg(sample_img)\n",
    "plt.figure()\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(sample_img)\n",
    "print(sample_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb9e745",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to load the file and convert it into a Tensor\n",
    "\n",
    "def load(image_file):\n",
    "    # Read and decode an image file to a uint8 tensor\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.io.decode_jpeg(image)\n",
    "\n",
    "    # Split each image tensor into two tensors:\n",
    "    # - one with visual image\n",
    "    # - one with the sensor scan \n",
    "    w = 1024\n",
    "    input_image = image[:, :w, :]\n",
    "    real_image = image[:, w:, :]\n",
    "\n",
    "    # Convert both images to float32 tensors\n",
    "    input_image = tf.cast(input_image, tf.float32)\n",
    "    real_image = tf.cast(real_image, tf.float32)\n",
    "\n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24305bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resizing the input image and the corresponding real image for processing\n",
    "\n",
    "inp, re = load(os.path.join(train_path, 'train_562.jpg'))\n",
    "inp = tf.image.resize(inp, [128, 256])\n",
    "re = tf.image.resize(re, [128, 256])\n",
    "\n",
    "plt.figure()\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inp/255.0)\n",
    "plt.figure()\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(re/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443cab36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting the training parameters\n",
    "\n",
    "# The training set contains ~ 1200 images\n",
    "BUFFER_SIZE = 1200\n",
    "\n",
    "# Baseline - A batch size of 1 produced better results in the original pix2pix paper\n",
    "# Batch-size-4x - A batch size of 4 is used for the second experiment\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Resize settings to the smaller image to maintain stability in output\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974b05ad",
   "metadata": {},
   "source": [
    "### Applying random jittering to preprocess the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df4282",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resizing the image randomly\n",
    "\n",
    "def resize(input_image, real_image, height, width):\n",
    "    input_image = tf.image.resize(input_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    real_image = tf.image.resize(real_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93b829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cropping out random patches from the image\n",
    "\n",
    "def random_crop(input_image, real_image):\n",
    "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "    cropped_image = tf.image.random_crop(stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "    \n",
    "    return cropped_image[0], cropped_image[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce18c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalising the image pixel values\n",
    "\n",
    "def normalise(input_image, real_image):\n",
    "    input_image = (input_image/127.5)-1\n",
    "    real_image = (real_image/127.5)-1\n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc5cd35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mirroring the image\n",
    "\n",
    "@tf.function()\n",
    "def random_jitter(input_image, real_image):\n",
    "    \n",
    "    input_image, real_image = resize(input_image, real_image, 128, 256) # Original size = \n",
    "    \n",
    "    input_image, real_image = random_crop(input_image, real_image)\n",
    "    \n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        real_image = tf.image.flip_left_right(real_image)\n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a6427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualising a few preprocessed images from the training set\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "for i in range(4):\n",
    "    rj_inp, rj_re = random_jitter(inp, re)\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.imshow(rj_inp/255.0)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be68ae09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading helper function for the training images\n",
    "\n",
    "def load_image_train(image_file):\n",
    "    input_image, real_image = load(image_file)\n",
    "    input_image, real_image = random_jitter(input_image, real_image)\n",
    "    input_image, real_image = normalise(input_image, real_image)\n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4568b7ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading helper function for the testing images\n",
    "\n",
    "def load_image_test(image_file):\n",
    "    input_image, real_image = load(image_file)\n",
    "    input_image, real_image = resize(input_image, real_image, IMG_HEIGHT, IMG_WIDTH)\n",
    "    input_image, real_image = normalise(input_image, real_image)\n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d4596",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building an input pipeline for training data\n",
    "\n",
    "train_dataset = tf.data.Dataset.list_files(os.path.join(train_path, '*.jpg'))\n",
    "train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d82f5fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building an input pipeline for testing data\n",
    "\n",
    "file_names = natsorted(os.listdir(test_path))\n",
    "file_paths = [os.path.join(test_path, file_name) for file_name in file_names if file_name.endswith('.jpg')]\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(file_paths)\n",
    "test_dataset = test_dataset.map(load_image_test)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c593a565",
   "metadata": {},
   "source": [
    "### Building the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405de487",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6e9bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The downsampling function of the Generator\n",
    "\n",
    "def downsample(filters, size, apply_batchnorm = True):\n",
    "    initialiser = tf.random_normal_initializer(0.,0.02)\n",
    "    \n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=initialiser, use_bias=False))\n",
    "    \n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "    return result\n",
    "\n",
    "down_model = downsample(3,4)\n",
    "down_result = down_model(tf.expand_dims(inp, 0))\n",
    "\n",
    "print(down_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f48dfac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The upsampling function of the Generator\n",
    "\n",
    "def upsample(filters, size, apply_dropout = False):\n",
    "    initialiser = tf.random_normal_initializer(0.,0.02)\n",
    "    \n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer=initialiser, use_bias=False))\n",
    "    \n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "    \n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "    \n",
    "    return result\n",
    "\n",
    "up_model = upsample(3,4)\n",
    "up_result = up_model(down_result)\n",
    "\n",
    "print(up_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fabee9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building the Generator with the downsampling and upsampling functions\n",
    "\n",
    "def Generator():\n",
    "    inputs = tf.keras.layers.Input(shape=[128, 256, 3])\n",
    "    \n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False),\n",
    "        downsample(128, 4),\n",
    "        downsample(256, 4),\n",
    "        downsample(512, 4),\n",
    "        downsample(512, 4),\n",
    "        downsample(512, 4),\n",
    "        downsample(512, 4),\n",
    "    ]\n",
    "    \n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True),\n",
    "        upsample(512, 4, apply_dropout=True),\n",
    "        upsample(512, 4, apply_dropout=True),\n",
    "        upsample(512, 4),\n",
    "        upsample(256, 4),\n",
    "        upsample(128, 4),\n",
    "        upsample(64, 4),\n",
    "    ]\n",
    "    \n",
    "    initialiser = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4, strides=2, padding='same', kernel_initializer=initialiser, activation='tanh')\n",
    "    \n",
    "    x = inputs\n",
    "    skips = []\n",
    "    \n",
    "    # Downsampling through the model\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "    \n",
    "    skips = reversed(skips[:-1])\n",
    "    \n",
    "    # Upsampling through the model\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])\n",
    "        \n",
    "    x = last(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0faa97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualising the Generator\n",
    "\n",
    "generator = Generator()\n",
    "tf.keras.utils.plot_model(generator, to_file='gen.png', show_shapes=True, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690497d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing the features learned by the Generator on the sample training image\n",
    "\n",
    "gen_output = generator(inp[tf.newaxis, ...], training=False)\n",
    "plt.imshow(gen_output[0, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f5c082",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining the Generator loss\n",
    "\n",
    "LAMBDA = 100\n",
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "    \n",
    "    # Mean absolute error\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target-gen_output))\n",
    "    \n",
    "    total_gen_loss = gan_loss + (LAMBDA*l1_loss)\n",
    "    \n",
    "    return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ade1f",
   "metadata": {},
   "source": [
    "### Building the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40976a16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building the Discriminator by using a PatchGAN architecture\n",
    "\n",
    "def Discriminator():\n",
    "    initialiser = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "    inp = tf.keras.layers.Input(shape=[128, 256, 3], name='input_image')\n",
    "    tar = tf.keras.layers.Input(shape=[128, 256, 3], name='target_image')\n",
    "    \n",
    "    x = tf.keras.layers.concatenate([inp, tar])\n",
    "    \n",
    "    down1 = downsample(64, 4, False)(x)\n",
    "    down2 = downsample(128, 4)(down1)\n",
    "    down3 = downsample(256, 4)(down2)\n",
    "    \n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)\n",
    "    conv = tf.keras.layers.Conv2D(512, 4, strides=1, kernel_initializer=initialiser, use_bias=False)(zero_pad1)\n",
    "    \n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "    \n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "    \n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)\n",
    "    \n",
    "    last = tf.keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=initialiser)(zero_pad2)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f078489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualising the Discriminator \n",
    "\n",
    "discriminator = Discriminator()\n",
    "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35184f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing the Discriminator on the sample image\n",
    "\n",
    "disc_out = discriminator([inp[tf.newaxis, ...], gen_output], training=False)\n",
    "plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ca964",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining the Discriminator loss\n",
    "\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "    \n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "    \n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    \n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96586cfd",
   "metadata": {},
   "source": [
    "### Optimisers and Checkpoint saving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ff197",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining Generator and Discriminator optimiser\n",
    "\n",
    "generator_optimiser = tf.keras.optimizers.Adam(2e-4, beta_1=0.6)\n",
    "discriminator_optimiser = tf.keras.optimizers.Adam(2e-4, beta_1=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df8db4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining checkpointts to save model weights\n",
    "\n",
    "checkpoint_dir = 'training_checkpoints_v1'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimiser, discriminator_optimizer=discriminator_optimiser, generator=generator, discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3862dae3",
   "metadata": {},
   "source": [
    "### Function to plot images during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705d03c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to visualise images while training\n",
    "\n",
    "def generate_images(model, test_input, tar):\n",
    "  prediction = model(test_input, training=True)\n",
    "  plt.figure(figsize=(15, 15))\n",
    "\n",
    "  display_list = [test_input[0], tar[0], prediction[0]]\n",
    "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "  for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.title(title[i])\n",
    "    # Getting the pixel values in the [0, 1] range to plot.\n",
    "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136490f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualisining the first step of training\n",
    "\n",
    "for example_input, example_target in test_dataset.take(1):\n",
    "    generate_images(generator, example_input, example_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d284f1",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f3de1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building the train step iterator as a helper function\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_image, target, step):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(input_image, training=True)\n",
    "\n",
    "        disc_real_output = discriminator([input_image, target], training=True)\n",
    "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimiser.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "    discriminator_optimiser.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_total_loss, disc_loss\n",
    "    \n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n",
    "        tf.summary.scalar('disc_loss', disc_loss, step=step//1000)\n",
    "\n",
    "\n",
    "# Visualising adversarial losses\n",
    "\n",
    "def show_losses(gen_total_loss, disc_loss):\n",
    "    print(\"generator loss: \", gen_total_loss.numpy())\n",
    "    print(\"Discriminator loss: \", disc_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431fed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logs to visualise training metrics using Tensorflow\n",
    "\n",
    "log_dir=\"logs/\"\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c1d8e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "def fit(train_ds, test_ds, steps):\n",
    "    example_input, example_target = next(iter(test_ds.take(1)))\n",
    "    start = time.time()\n",
    "    \n",
    "    i = 1\n",
    "    j = 100\n",
    "\n",
    "    for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n",
    "        if (step) % 1000 == 0:\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "        if step != 0:\n",
    "            print(f'Time taken for step no. {step} - {time.time()-start:.2f} sec\\n')\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        generate_images(generator, example_input, example_target)\n",
    "        print(f\"Step: {step//1000}k\\n\")\n",
    "\n",
    "        train_step(input_image, target, step)\n",
    "\n",
    "\n",
    "        if (step+1) % 100 == 0:\n",
    "            print(j, ' steps done\\n', end='', flush=True)\n",
    "            j=j+100\n",
    "            gen_output = generator(input_image, training=True)\n",
    "            disc_real_output = discriminator([input_image, target], training=True)\n",
    "            disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "            gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "            disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "            show_losses(gen_total_loss, disc_loss)\n",
    "            \n",
    "\n",
    "        if (step+1) % 400 == 0:\n",
    "            print(i, ' epochs completed\\n', end='', flush=True)\n",
    "            i= i+1\n",
    "    \n",
    "\n",
    "        # Save (checkpoint) the model every 5k steps\n",
    "        if (step + 1) % 5000 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8258ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Logging information to tensorboard and visualising training metrics\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {'logsfit'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f177507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the availability of GPU\n",
    "\n",
    "tf.config.experimental.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262fb9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if a saved checkpoint or model weight exists\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10474013",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Starting the training\n",
    "\n",
    "fit(train_dataset, test_dataset, steps=40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5e3718",
   "metadata": {},
   "source": [
    "### Testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bc6959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the model to generate a few visualisations on testing set\n",
    "\n",
    "for inp, tar in test_dataset.take(5):\n",
    "    generate_images(generator, inp, tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6087a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to apply the model on the test dataset directory and the save the output\n",
    "# The output is saved in same visual format as input with concatenated scan and generated image\n",
    "\n",
    "def generate_images_test(model, test_input, tar):\n",
    "    prediction = model(test_input, training=True)\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "    display_list = [test_input[0], prediction[0]]\n",
    "\n",
    "    # Concatenating the images\n",
    "    concatenated_image = np.concatenate(display_list, axis=1)\n",
    "\n",
    "    # Displaying the concatenated image\n",
    "    plt.imshow(concatenated_image * 0.5 + 0.5)\n",
    "    \n",
    "    # Adjusting the size of the axes to fill the entire figure\n",
    "    plt.subplots_adjust(top=1, bottom=0, right=1, left=0, hspace=0, wspace=0)\n",
    "    plt.margins(0, 0)\n",
    "    \n",
    "    # Removing the axis\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Saving the image to a buffer\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='jpg', bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    # Converting the buffer to an image\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def save_images_test(model, test_dataset, directory, frames_test_dir, size=(512, 150)):\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    # Get the list of file names in the frames_test directory\n",
    "    file_names = natsorted(os.listdir(frames_test_dir))\n",
    "    \n",
    "    # Iterate over the test dataset\n",
    "    for i, (inp, tar) in tqdm(enumerate(test_dataset)):\n",
    "        # Generate the image\n",
    "        img = generate_images_test(model, inp, tar)\n",
    "        \n",
    "        # Resize the image\n",
    "        img = img.resize(size, Image.ANTIALIAS)\n",
    "        \n",
    "        # Get the file name for this image\n",
    "        file_name = file_names[i]\n",
    "        \n",
    "        # Save the image to the specified directory\n",
    "        img.save(os.path.join(directory, file_name))\n",
    "\n",
    "save_images_test(generator, test_dataset, 'sonar_data_augmented', 'sonar_data_test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
